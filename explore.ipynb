{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config - imports, constants, and mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import astropy\n",
    "from astropy.io import fits\n",
    "from astropy.table import Table\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "\n",
    "output_dir = \"../output/\"\n",
    "\n",
    "\"\"\" \n",
    "The groupings below map specific data-based claimed types to larger \n",
    "groupings. For example: nIa, Ia, Ia*, and Ia-HV all map to Ia.\n",
    "Any new claimed types to be considered in the analysis need to be added \n",
    "here and mapped to a larger group, which also needs to be in cat_code\n",
    "\"\"\"\n",
    "groupings = {\n",
    "#     Ia\n",
    "    'nIa' : 'Ia',\n",
    "    'Ia' : 'Ia',\n",
    "    'Ia*' :'Ia',\n",
    "    'Ia-HV' :'Ia',\n",
    "#     'Ia-91T'\n",
    "    'Ia-91T' : 'Ia-91T',\n",
    "#     'Ia-91bg'\n",
    "    'Ia-91bg' : 'Ia-91bg',\n",
    "#     'Ib/Ic'\n",
    "    'IIb/Ib/Ic (Ca rich)' : 'Ib/Ic',\n",
    "    'Ib/Ic (Ca rich)' : 'Ib/Ic',\n",
    "    'Ib (Ca rich)' : 'Ib/Ic',\n",
    "#     'Ia CSM'\n",
    "    'Ia CSM' : 'Ia CSM',\n",
    "#     'Ia-02cx'\n",
    "    'Ia-02cx' : 'Ia-02cx',\n",
    "    'Iax[02cx-like]' : 'Ia-02cx',\n",
    "#     'II P'\n",
    "    'II P' : 'II P',\n",
    "    'II-p' : 'II P',\n",
    "    'II Pec' : 'II P',\n",
    "    'II P Pec' : 'II P',\n",
    "    'II?' : 'II P',\n",
    "    'II Pec?' : 'II P',\n",
    "    'IIP?' : 'II P',\n",
    "    'II' : 'II P',\n",
    "#     'IIn'\n",
    "    'IIn' : 'IIn',\n",
    "    'IIn Pec' : 'IIn',\n",
    "    'LBV to IIn' : 'IIn',\n",
    "    'IIn-pec/LBV' : 'IIn',\n",
    "    'IIn?' : 'IIn',\n",
    "#     'Ib'\n",
    "    'Ib' : 'Ib',\n",
    "    'Ibn' : 'Ib',\n",
    "#     'Ic'\n",
    "    'Ic' : 'Ic',\n",
    "    'Ic?' : 'Ic',\n",
    "    'Ic-lum?' : 'Ic',\n",
    "    'Ic Pec' : 'Ic',\n",
    "#     Ic BL\n",
    "    'Ic BL' : 'Ic BL',\n",
    "    'BL-Ic' : 'Ic BL',\n",
    "#     'SLSN-II?'\n",
    "    'SLSN-II?' : 'SLSN-II?',\n",
    "#     'SLSN-I'\n",
    "    'SLSN-I' : 'SLSN-I',\n",
    "#     'LGRB'\n",
    "    'LGRB' : 'LGRB'\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Claimed Type categories, mapped to integer values.\n",
    "The integer values are used by the ML model.\n",
    "\"\"\" \n",
    "cat_code = {\n",
    "    'LGRB': 1,\n",
    "    'Ia' : 2, \n",
    "    'II P': 3, \n",
    "    'Ib' : 4,\n",
    "    'Ic' : 5, \n",
    "    'IIn': 6, \n",
    "    'SLSN-I' : 7, \n",
    "    'Ia-02cx' : 8,\n",
    "    'Ic BL' : 9, \n",
    "    'SLSN-II?': 10, \n",
    "    'Ia-91bg' : 11, \n",
    "    'Ia-91T' : 12, \n",
    "    'Ia CSM' : 13\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert .fits data to Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(file = '../data/osc+otc-Assembled.fits'):\n",
    "    dat = Table.read(file, format='fits')\n",
    "    df_bytes = dat.to_pandas()  # Convert to pandas dataframe\n",
    "    df = pd.DataFrame()     # Init empty dataframe for converted types\n",
    "\n",
    "    # Convert byte columns to strings\n",
    "    for column in df_bytes:\n",
    "        if df_bytes[column].dtype == np.dtype('object'):\n",
    "            df[column + \"_str\"] = df_bytes[column].str.decode(\"utf-8\")\n",
    "            df[column] = df[column + \"_str\"].copy()\n",
    "            df.drop(column + \"_str\", axis = 1, inplace = True)\n",
    "        else:\n",
    "            df[column] = df_bytes[column]\n",
    "\n",
    "    # Prints sum of NULL values by column\n",
    "    df.isnull().sum().to_csv(output_dir + \"Missing_Values.csv\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Group by Claimed Type and Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_cts(valid_df):\n",
    "    # Group claimed types into supergroups, defined in groupings dict\n",
    "    valid_df['claimedtype_group'] = valid_df.apply(lambda row:  \n",
    "                                             groupings[row.claimedtype] \n",
    "                                             if row.claimedtype in groupings \n",
    "                                             else None,\n",
    "                                             axis=1)\n",
    "\n",
    "    # Dataframe of claimed types that do not have group\n",
    "    ungrouped_types = list(set(valid_df.claimedtype.unique()) - set(groupings.keys()))\n",
    "    print(str(len(ungrouped_types)) + \" rows with ungrouped claimed type.\")\n",
    "\n",
    "    # Claimed Types that need to be grouped\n",
    "    add_cts = []\n",
    "    for ct in list(valid_df.claimedtype.unique()):\n",
    "        if \",\" not in ct and ct != '' and ct not in groupings:\n",
    "            add_cts.append(ct)\n",
    "    # Drop rows with no grouped claimed type      \n",
    "    valid_df = valid_df[~valid_df['claimedtype_group'].isnull()]\n",
    "    print (\"Remaining rows in valid dataframe: \" + str(valid_df.shape[0]))\n",
    "    \n",
    "    return valid_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import sklearn.metrics as skm\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up source and target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_sample(df, oversampled_val, undersampled_val, col_val):\n",
    "    \"\"\"\n",
    "    Sub-samples over-represented class\n",
    "    :param df: the dataframe to manipulate\n",
    "    :param oversampled_val: the value of the class that is over-represented\n",
    "    :param col_val: the column name for the value\n",
    "    \"\"\"\n",
    "    oversample = df[df[col_val] == oversampled_val]\n",
    "    undersample = df[df[col_val] == undersampled_val]\n",
    "    keep_oversampled = oversample.sample(n = undersample.shape[0])\n",
    "    \n",
    "    remaining = df[(df[col_val] != undersampled_val) & (df[col_val] != oversampled_val)]\n",
    "    \n",
    "    sub_df = pd.concat([keep_oversampled, undersample, remaining])\n",
    "#     print(\"Number of undersampled = new oversampled = \" + str(undersample.shape[0]))\n",
    "#     print(\"Number of rows in subsample is: \" + str(sub_df.shape[0]))\n",
    "    return pd.concat([keep_oversampled, undersample, remaining])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Target dataframe\n",
    "def set_up_target(df_rs_band):\n",
    "    df_analyze = df_rs_band.copy()\n",
    "    # Split out claimedtype_group into new dataframe\n",
    "    y = df_analyze.claimedtype_group.to_frame(name='group')\n",
    "    # Use category code numbers from cat_code dict, instead of strings\n",
    "    y['cat_code'] = y.apply(lambda row:  cat_code[row.group] , axis=1)\n",
    "    y = y.drop('group', axis = 1)\n",
    "    return y\n",
    "\n",
    "\n",
    "def set_up_source(df_analyze):\n",
    "    X = df_analyze.copy()\n",
    "    X = X.drop(['redshift', 'claimedtype_group'], axis = 1)\n",
    "        \n",
    "    # Encode all letters as numbers\n",
    "    le = LabelEncoder()\n",
    "    encoded_series = X[X.columns[:]].apply(le.fit_transform)\n",
    "    X = encoded_series\n",
    "    \n",
    "    return X\n",
    "\n",
    "def split_train_test(X, y):\n",
    "    # Split Training and Testing Data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=2)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run_rm(X_train, X_test, y_train, y_test):\n",
    "    clf = RandomForestClassifier(max_depth = 8, random_state = 0, class_weight = \"balanced\")\n",
    "    clf.fit(X_train, y_train)\n",
    "    predictions = clf.predict(X_test)\n",
    "    return clf, predictions\n",
    "\n",
    "def get_performance(y_test, predictions):\n",
    "    return classification_report(y_test, predictions)\n",
    "\n",
    "def get_feature_importance(clf, X_train):\n",
    "    feature_importances = pd.DataFrame(clf.feature_importances_,\n",
    "                                       index = X_train.columns,\n",
    "                                       columns=['importance'])\n",
    "    feature_importances = feature_importances.sort_values('importance', \n",
    "                                                          ascending = False)\n",
    "    return feature_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(data_df, col_list, min_redshift, max_redshift, subsample):\n",
    "    grouped_df = group_cts(data_df)\n",
    "    valid_df = grouped_df.copy()\n",
    "    \n",
    "#     Filter on specific list of columns to consider\n",
    "    valid_df = valid_df[col_list + ['redshift', 'claimedtype_group']]\n",
    "    \n",
    "    df_rs_band = valid_df.loc[(valid_df.redshift > min_redshift) \n",
    "                              & (valid_df.redshift <= max_redshift)]\n",
    "#     Subsample Ia to II P level\n",
    "    if subsample:\n",
    "        df_rs_band = sub_sample(df = df_rs_band, \n",
    "                                oversampled_val = 'Ia', \n",
    "                                undersampled_val = 'II P', \n",
    "                                col_val = 'claimedtype_group')  \n",
    "    df_rs_band = df_rs_band.dropna()\n",
    "    \n",
    "#     Run through ML\n",
    "    y = set_up_target(df_rs_band)\n",
    "    X = set_up_source(df_rs_band)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def run_analysis(data_df, col_list, min_redshift, max_redshift, subsample = True):\n",
    "    X, y = prep_data(data_df, col_list, \n",
    "                     min_redshift, \n",
    "                     max_redshift, subsample)\n",
    "    print(\"Rows run through classifier in training data: \" + str(X.shape[0]))\n",
    "    X_train, X_test, y_train, y_test = split_train_test(X, y)\n",
    "\n",
    "    clf, predictions = run_rm(X_train, X_test, y_train, y_test)\n",
    "    get_feature_importance(clf, X_train)\n",
    "    print(classification_report(y_test, predictions))\n",
    "    return clf, X_train, X_test, y_test, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the entire analysis using the cells below. Uses a sample of the original data with redshift within min_redshift and max_redshift, and using only fields in the col_list of the analysis. \n",
    "\n",
    "col_list - the list of data fields to use in the machine learning algorithm. They have been subdivided into spectral columns (available in the cell below). The reason for this is to consider only the spectra in the analysis. Considering any other fields leads to a higher risk of having a biased algorithm favoring redshift-driven characteristics. \n",
    "\n",
    "min_redshift - The minimum redshift to consider for the data sample, exclusive\n",
    "\n",
    "max_redshift - The maximum redshift to consider for the data sample, inclusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = collect_data(file = '../data/THEx-catalog.v0_0_3.fits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ned_mass_cols = ['NED_2MASS_J', 'NED_2MASS_H', 'NED_2MASS_Ks']\n",
    "\n",
    "ned_sdss_cols = ['NED_SDSS_u', 'NED_SDSS_g', 'NED_SDSS_r', 'NED_SDSS_i', 'NED_SDSS_z']\n",
    "\n",
    "ned_galex_cols = ['NED_GALEX_NUV', 'NED_GALEX_FUV']\n",
    "\n",
    "ned_iras_cols = ['NED_IRAS_12m', 'NED_IRAS_25m', 'NED_IRAS_60m', 'NED_IRAS_100m', 'NED_HI_21cm', 'NED_1p4GHz']\n",
    "\n",
    "# allwise_cols = ['AllWISE_W1', 'AllWISE_W2', 'AllWISE_W3', 'AllWISE_W4']\n",
    "\n",
    "firefly_cols = [col for col in list(data_df) if \"Firefly\" in col] # MPAJHU\n",
    "\n",
    "mpa_cols = [col for col in list(data_df) if \"MPAJHU\" in col] # MPAJHU\n",
    "# HyperLEDA\n",
    "\n",
    "# GalaxyZoo\n",
    "zoo_cols = [col for col in list(data_df) if \"Zoo\" in col] # MPAJHU\n",
    "\n",
    "zoo2 = [col for col in list(data_df) if \"GalaxyZoo2\" in col] # galaxy zoo 2\n",
    "\n",
    "gswlc = [col for col in list(data_df) if \"GSWLC\" in col] # GSWLC\n",
    "\n",
    "# bad\n",
    "wiscpca = [col for col in list(data_df) if \"WiscPCA\" in col] # WiscPCA\n",
    "\n",
    "nsa = [col for col in list(data_df) if \"NSA_\" in col] # NSA\n",
    "\n",
    "# scos - 50% PERFORMANCE, bad\n",
    "scos = [col for col in list(data_df) if \"SCOS_\" in col] # SCOS\n",
    "\n",
    "# bad\n",
    "ps1 = [col for col in list(data_df) if \"PS1\" in col] # PS1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288 rows with ungrouped claimed type.\n",
      "Remaining rows in valid dataframe: 15483\n",
      "Rows run through classifier in training data: 1480\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          2       0.42      0.53      0.46       137\n",
      "          3       0.61      0.54      0.57       274\n",
      "          4       0.00      0.00      0.00        22\n",
      "          5       0.12      0.10      0.11        39\n",
      "          6       0.00      0.00      0.00        12\n",
      "          9       0.00      0.00      0.00         1\n",
      "         11       0.00      0.00      0.00         2\n",
      "         12       0.00      0.00      0.00         2\n",
      "\n",
      "avg / total       0.47      0.46      0.46       489\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marina/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/marina/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "clf, X_train, X_test, y_test, predictions = run_analysis(data_df = data_df,\n",
    "                            col_list = nsa,  \n",
    "                            min_redshift = 0, \n",
    "                            max_redshift = 1,\n",
    "                            subsample = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try other classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288 rows with ungrouped claimed type.\n",
      "Remaining rows in valid dataframe: 15483\n"
     ]
    }
   ],
   "source": [
    "X,y = prep_data(data_df = data_df,\n",
    "                            col_list = nsa,  \n",
    "                            min_redshift = 0, \n",
    "                            max_redshift = 1,\n",
    "                            subsample = True)\n",
    "X_train, X_test, y_train, y_test = split_train_test(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          2       0.53      0.46      0.49       138\n",
      "          3       0.62      0.81      0.70       273\n",
      "          4       0.00      0.00      0.00        29\n",
      "          5       0.00      0.00      0.00        31\n",
      "          6       0.00      0.00      0.00        11\n",
      "          9       0.00      0.00      0.00         1\n",
      "         11       0.00      0.00      0.00         3\n",
      "         12       0.00      0.00      0.00         5\n",
      "\n",
      "avg / total       0.49      0.58      0.53       491\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marina/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/marina/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0, solver='lbfgs',\n",
    "                          multi_class='multinomial').fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "clf.score(X_test, y_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marina/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(kernel='linear', class_weight='balanced').fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "clf.score(X_test, y_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision Improver -- Attempt to improve precision by upping the probability necessary to classify something as a particular class. Adds Other class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other: 1122\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00         0\n",
      "          2       0.00      0.00      0.00       421\n",
      "          3       0.00      0.00      0.00       523\n",
      "          4       0.00      0.00      0.00        41\n",
      "          5       0.00      0.00      0.00        75\n",
      "          6       0.00      0.00      0.00        42\n",
      "          7       0.00      0.00      0.00         1\n",
      "          8       0.00      0.00      0.00         2\n",
      "          9       0.00      0.00      0.00         4\n",
      "         11       0.00      0.00      0.00         1\n",
      "         12       0.00      0.00      0.00        12\n",
      "\n",
      "avg / total       0.00      0.00      0.00      1122\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marina/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/marina/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "pps = clf.predict_proba(X_test)\n",
    "adj_results = []\n",
    "for probs in pps:\n",
    "    best_class = 0\n",
    "    for idx, val in enumerate(probs):\n",
    "        if val > 0.9:\n",
    "            best_class = clf.classes_[idx]\n",
    "    \n",
    "    adj_results.append(best_class)\n",
    "adj_preds = pd.DataFrame(adj_results, columns = ['class_pred'])\n",
    "\n",
    "print(\"Other: \" + str(adj_preds.loc[adj_preds.class_pred == 0].shape[0]))\n",
    "print(classification_report(y_test, adj_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra/Old Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_over_all_redshifts(sel_cols, valid_data):\n",
    "    for col_name in sel_cols:\n",
    "        valid_data = valid_data[valid_data[col_name].notnull()]\n",
    "\n",
    "    rs_mins = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "    rows = []\n",
    "    for rs_min in rs_mins:\n",
    "        rs_max = rs_min + 0.1\n",
    "        clf, X_train, y_test, predictions = run_analysis(data_df = data_df,\n",
    "                                col_list = sel_cols,  \n",
    "                                min_redshift = rs_min, \n",
    "                                max_redshift = rs_max)\n",
    "\n",
    "        df = get_feature_importance(clf, X_train)\n",
    "        col_vals = []\n",
    "        for col_name in sel_cols:\n",
    "            col_vals.append(df[df.index == col_name].importance[0])\n",
    "\n",
    "        types = list(range(1,14)) # list types to return, 1 to 13\n",
    "        recalls = list(recall_score(y_test, predictions, labels = types, average = None)) \n",
    "        ps = list(precision_score(y_test, predictions, labels = types, average = None)) \n",
    "        f1 = list(f1_score(y_test, predictions, labels = types, average = None))\n",
    "        class_2_recall = recalls[1]\n",
    "        class_3_recall = recalls[2]\n",
    "        class_2_ps = ps[1] \n",
    "        class_3_ps = ps[2]    \n",
    "        class_2_f1 = f1[1]\n",
    "        class_3_f1 = f1[2]\n",
    "        row = [rs_min, rs_max] + col_vals + [class_2_recall, class_3_recall, class_2_ps, class_3_ps, class_2_f1, class_3_f1]\n",
    "        rows.append(row)\n",
    "        \n",
    "    return rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_cols = mpa_cols\n",
    "rows = run_over_all_redshifts(cur_cols, data_df.copy())\n",
    "report = pd.DataFrame(rows, columns = ['min redshift', 'max redshift'] + cur_cols + ['Ia Recall',  'II P Recall', 'Ia Precision', 'II P Precision', 'Ia F1', 'II P F1'])\n",
    "report.round(decimals = 2).to_csv(\"../output/performance/mpa_cols.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Profiling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Feature Distribution by Claimed Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get distribution of features across claimed type group\n",
    "type_counts = pd.crosstab(df_rs_band['claimedtype_group'], \n",
    "                                    columns = [df_rs_band['masses'],\n",
    "                                              df_rs_band['velocity'],\n",
    "                                              df_rs_band['SDSS_SpecMatched'],\n",
    "                                              df_rs_band['SDSS_FiberID'],\n",
    "                                              df_rs_band['SDSS_MJD'],\n",
    "                                              df_rs_band['SDSS_Plate'],\n",
    "                                              df_rs_band['MPAJHU_BPTClass'],\n",
    "                                              df_rs_band['WiscPCA_src'],\n",
    "                                              df_rs_band['HyperLEDA_type'],\n",
    "                                              df_rs_band['HyperLEDA_bar'],\n",
    "                                              df_rs_band['HyperLEDA_ring'],\n",
    "                                               df_rs_band['HyperLEDA_multiple'],\n",
    "                                               df_rs_band['HyperLEDA_compactness'],\n",
    "                                               df_rs_band['HyperLEDA_agnclass']\n",
    "                                                ])\n",
    "type_counts.to_csv(\"feature_counts.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of the number of claimed types within each range of redshifts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group redshifts into bins of .5\n",
    "bin_size = 0.1\n",
    "rsmin = 0\n",
    "rsmax = 1\n",
    "range_bins = list(np.arange(int(rsmin)-1, int(rsmax) + 2, bin_size))\n",
    "# Create aggregate frame based on redshift and claimedtype_group\n",
    "\n",
    "grouped_df = group_cts(data_df)\n",
    "df_agg = grouped_df.copy()\n",
    "df_agg = df_agg[(df_agg.redshift != 'nan') & (df_agg.redshift.str.contains(',') == False)]\n",
    "df_agg['redshift'] = df_agg['redshift'].apply(pd.to_numeric)\n",
    "\n",
    "df_agg = df_agg[['claimedtype_group','redshift']]\n",
    "# Split counts into bins of redshift, of bin_size\n",
    "df_agg['rs_range'] = pd.cut(df_agg['redshift'], bins = range_bins)\n",
    "\n",
    "# Flip axis so each bin is a column\n",
    "grouping = df_agg.groupby(['claimedtype_group',\n",
    "                           'rs_range']).size().reset_index(name = 'counts')\n",
    "g = pd.DataFrame(grouping)\n",
    "col_headers = [df_agg.rs_range.unique()]\n",
    "ct_range = g.pivot(index = 'claimedtype_group', columns = 'rs_range')['counts']\n",
    "\n",
    "# Save claimedtype ranges to CSV\n",
    "# ct_range.to_csv(output_dir + \"ct_range_bin_\" + str(bin_size) + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_range"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
